<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_il0lzmq2iji5-2.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-2 0}.lst-kix_il0lzmq2iji5-0>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-0}ol.lst-kix_lo0h0uhovqi2-0{list-style-type:none}.lst-kix_il0lzmq2iji5-1>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-1}.lst-kix_a8g0yfe41h5n-4>li:before{content:"\0025cb   "}ul.lst-kix_lo0h0uhovqi2-8{list-style-type:none}ul.lst-kix_lo0h0uhovqi2-7{list-style-type:none}ul.lst-kix_lo0h0uhovqi2-6{list-style-type:none}.lst-kix_a8g0yfe41h5n-5>li:before{content:"\0025a0   "}.lst-kix_a8g0yfe41h5n-6>li:before{content:"\0025cf   "}.lst-kix_a8g0yfe41h5n-7>li:before{content:"\0025cb   "}ul.lst-kix_xlh8jlaybxo-6{list-style-type:none}ul.lst-kix_xlh8jlaybxo-7{list-style-type:none}ul.lst-kix_xlh8jlaybxo-8{list-style-type:none}.lst-kix_il0lzmq2iji5-3>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-3,decimal) ". "}.lst-kix_il0lzmq2iji5-5>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-5,lower-roman) ". "}.lst-kix_a8g0yfe41h5n-8>li:before{content:"\0025a0   "}.lst-kix_il0lzmq2iji5-4>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-4,lower-latin) ". "}.lst-kix_il0lzmq2iji5-8>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-8,lower-roman) ". "}ul.lst-kix_lo0h0uhovqi2-5{list-style-type:none}.lst-kix_il0lzmq2iji5-3>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-3}ul.lst-kix_lo0h0uhovqi2-4{list-style-type:none}ul.lst-kix_xlh8jlaybxo-0{list-style-type:none}ul.lst-kix_lo0h0uhovqi2-3{list-style-type:none}.lst-kix_il0lzmq2iji5-7>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-7,lower-latin) ". "}ul.lst-kix_xlh8jlaybxo-1{list-style-type:none}ul.lst-kix_lo0h0uhovqi2-2{list-style-type:none}ul.lst-kix_xlh8jlaybxo-2{list-style-type:none}ul.lst-kix_lo0h0uhovqi2-1{list-style-type:none}ul.lst-kix_xlh8jlaybxo-3{list-style-type:none}ul.lst-kix_xlh8jlaybxo-4{list-style-type:none}.lst-kix_il0lzmq2iji5-6>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-6,decimal) ". "}ul.lst-kix_xlh8jlaybxo-5{list-style-type:none}.lst-kix_bkuxn1vgrcem-8>li:before{content:"\0025cf   "}.lst-kix_bkuxn1vgrcem-6>li:before{content:"\0025cf   "}.lst-kix_bkuxn1vgrcem-5>li:before{content:"\0025cf   "}.lst-kix_bkuxn1vgrcem-4>li:before{content:"\0025cf   "}.lst-kix_xlh8jlaybxo-0>li:before{content:"\0025cf   "}.lst-kix_bkuxn1vgrcem-3>li:before{content:"\0025cf   "}ol.lst-kix_il0lzmq2iji5-7.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-7 0}.lst-kix_xlh8jlaybxo-2>li:before{content:"\0025a0   "}.lst-kix_xlh8jlaybxo-4>li:before{content:"\0025cb   "}.lst-kix_xlh8jlaybxo-1>li:before{content:"\0025cb   "}.lst-kix_xlh8jlaybxo-5>li:before{content:"\0025a0   "}.lst-kix_bkuxn1vgrcem-7>li:before{content:"\0025cf   "}.lst-kix_xlh8jlaybxo-3>li:before{content:"\0025cf   "}.lst-kix_jeohxv44ijje-8>li:before{content:"\0025a0   "}.lst-kix_il0lzmq2iji5-8>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-8}ul.lst-kix_jeohxv44ijje-0{list-style-type:none}ul.lst-kix_jeohxv44ijje-1{list-style-type:none}.lst-kix_xlh8jlaybxo-6>li:before{content:"\0025cf   "}.lst-kix_xlh8jlaybxo-8>li:before{content:"\0025a0   "}ul.lst-kix_jeohxv44ijje-2{list-style-type:none}.lst-kix_il0lzmq2iji5-2>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-2}.lst-kix_il0lzmq2iji5-5>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-5}ol.lst-kix_il0lzmq2iji5-1.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-1 0}.lst-kix_xlh8jlaybxo-7>li:before{content:"\0025cb   "}.lst-kix_bkuxn1vgrcem-0>li:before{content:"\0025cf   "}ul.lst-kix_jeohxv44ijje-7{list-style-type:none}ul.lst-kix_jeohxv44ijje-8{list-style-type:none}.lst-kix_bkuxn1vgrcem-2>li:before{content:"\0025cf   "}ul.lst-kix_jeohxv44ijje-3{list-style-type:none}ul.lst-kix_jeohxv44ijje-4{list-style-type:none}.lst-kix_bkuxn1vgrcem-1>li:before{content:"\0025cf   "}ul.lst-kix_jeohxv44ijje-5{list-style-type:none}ul.lst-kix_jeohxv44ijje-6{list-style-type:none}ol.lst-kix_il0lzmq2iji5-8.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-8 0}ol.lst-kix_il0lzmq2iji5-1{list-style-type:none}ol.lst-kix_il0lzmq2iji5-2{list-style-type:none}ol.lst-kix_il0lzmq2iji5-0{list-style-type:none}ol.lst-kix_il0lzmq2iji5-5{list-style-type:none}ol.lst-kix_il0lzmq2iji5-6{list-style-type:none}ol.lst-kix_il0lzmq2iji5-3{list-style-type:none}ol.lst-kix_il0lzmq2iji5-4{list-style-type:none}ul.lst-kix_m3hcf1tp6dx0-1{list-style-type:none}ul.lst-kix_m3hcf1tp6dx0-2{list-style-type:none}ul.lst-kix_m3hcf1tp6dx0-0{list-style-type:none}.lst-kix_m3hcf1tp6dx0-2>li:before{content:"\0025a0   "}.lst-kix_il0lzmq2iji5-7>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-7}ul.lst-kix_bkuxn1vgrcem-7{list-style-type:none}ul.lst-kix_m3hcf1tp6dx0-5{list-style-type:none}.lst-kix_jeohxv44ijje-2>li:before{content:"\0025a0   "}ul.lst-kix_bkuxn1vgrcem-6{list-style-type:none}ul.lst-kix_m3hcf1tp6dx0-6{list-style-type:none}ul.lst-kix_bkuxn1vgrcem-5{list-style-type:none}ul.lst-kix_m3hcf1tp6dx0-3{list-style-type:none}.lst-kix_jeohxv44ijje-3>li:before{content:"\0025cf   "}ul.lst-kix_bkuxn1vgrcem-4{list-style-type:none}.lst-kix_m3hcf1tp6dx0-3>li:before{content:"\0025cf   "}ul.lst-kix_m3hcf1tp6dx0-4{list-style-type:none}ul.lst-kix_bkuxn1vgrcem-3{list-style-type:none}.lst-kix_jeohxv44ijje-4>li:before{content:"\0025cb   "}ul.lst-kix_bkuxn1vgrcem-2{list-style-type:none}ul.lst-kix_bkuxn1vgrcem-1{list-style-type:none}ul.lst-kix_m3hcf1tp6dx0-7{list-style-type:none}ul.lst-kix_bkuxn1vgrcem-0{list-style-type:none}.lst-kix_m3hcf1tp6dx0-4>li:before{content:"\0025cb   "}ul.lst-kix_m3hcf1tp6dx0-8{list-style-type:none}.lst-kix_jeohxv44ijje-6>li:before{content:"\0025cf   "}.lst-kix_jeohxv44ijje-5>li:before{content:"\0025a0   "}.lst-kix_jeohxv44ijje-7>li:before{content:"\0025cb   "}.lst-kix_m3hcf1tp6dx0-5>li:before{content:"\0025a0   "}.lst-kix_m3hcf1tp6dx0-7>li:before{content:"\0025cb   "}.lst-kix_m3hcf1tp6dx0-6>li:before{content:"\0025cf   "}.lst-kix_yf08h6pwef38-2>li:before{content:"\0025a0   "}.lst-kix_yf08h6pwef38-3>li:before{content:"\0025cf   "}.lst-kix_m3hcf1tp6dx0-8>li:before{content:"\0025a0   "}.lst-kix_yf08h6pwef38-6>li:before{content:"\0025cf   "}.lst-kix_yf08h6pwef38-4>li:before{content:"\0025cb   "}.lst-kix_yf08h6pwef38-5>li:before{content:"\0025a0   "}ul.lst-kix_bkuxn1vgrcem-8{list-style-type:none}.lst-kix_jeohxv44ijje-1>li:before{content:"\0025cb   "}ol.lst-kix_il0lzmq2iji5-6.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-6 0}.lst-kix_jeohxv44ijje-0>li:before{content:"\0025cf   "}.lst-kix_yf08h6pwef38-7>li:before{content:"\0025cb   "}ol.lst-kix_lo0h0uhovqi2-0.start{counter-reset:lst-ctn-kix_lo0h0uhovqi2-0 0}ol.lst-kix_il0lzmq2iji5-0.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-0 0}ol.lst-kix_il0lzmq2iji5-7{list-style-type:none}ol.lst-kix_il0lzmq2iji5-8{list-style-type:none}ol.lst-kix_il0lzmq2iji5-3.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-3 0}.lst-kix_il0lzmq2iji5-6>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-6}.lst-kix_yf08h6pwef38-8>li:before{content:"\0025a0   "}.lst-kix_lo0h0uhovqi2-5>li:before{content:"\0025a0   "}ol.lst-kix_il0lzmq2iji5-4.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-4 0}.lst-kix_lo0h0uhovqi2-4>li:before{content:"\0025cb   "}.lst-kix_lo0h0uhovqi2-6>li:before{content:"\0025cf   "}.lst-kix_lo0h0uhovqi2-3>li:before{content:"\0025cf   "}.lst-kix_lo0h0uhovqi2-7>li:before{content:"\0025cb   "}.lst-kix_il0lzmq2iji5-0>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-0,decimal) ". "}.lst-kix_lo0h0uhovqi2-1>li:before{content:"\0025cb   "}.lst-kix_lo0h0uhovqi2-0>li:before{content:"" counter(lst-ctn-kix_lo0h0uhovqi2-0,decimal) ". "}.lst-kix_lo0h0uhovqi2-2>li:before{content:"\0025a0   "}.lst-kix_lo0h0uhovqi2-8>li:before{content:"\0025a0   "}.lst-kix_il0lzmq2iji5-1>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-1,lower-latin) ". "}.lst-kix_il0lzmq2iji5-2>li:before{content:"" counter(lst-ctn-kix_il0lzmq2iji5-2,lower-roman) ". "}.lst-kix_il0lzmq2iji5-4>li{counter-increment:lst-ctn-kix_il0lzmq2iji5-4}.lst-kix_lo0h0uhovqi2-0>li{counter-increment:lst-ctn-kix_lo0h0uhovqi2-0}.lst-kix_yf08h6pwef38-0>li:before{content:"\0025cf   "}.lst-kix_a8g0yfe41h5n-3>li:before{content:"\0025cf   "}.lst-kix_yf08h6pwef38-1>li:before{content:"\0025cb   "}.lst-kix_a8g0yfe41h5n-1>li:before{content:"\0025cb   "}.lst-kix_a8g0yfe41h5n-2>li:before{content:"\0025a0   "}ul.lst-kix_a8g0yfe41h5n-5{list-style-type:none}.lst-kix_a8g0yfe41h5n-0>li:before{content:"\0025cf   "}ul.lst-kix_a8g0yfe41h5n-6{list-style-type:none}ul.lst-kix_a8g0yfe41h5n-7{list-style-type:none}ul.lst-kix_a8g0yfe41h5n-8{list-style-type:none}ul.lst-kix_yf08h6pwef38-5{list-style-type:none}ul.lst-kix_yf08h6pwef38-6{list-style-type:none}ul.lst-kix_yf08h6pwef38-7{list-style-type:none}ol.lst-kix_il0lzmq2iji5-5.start{counter-reset:lst-ctn-kix_il0lzmq2iji5-5 0}ul.lst-kix_yf08h6pwef38-8{list-style-type:none}ul.lst-kix_yf08h6pwef38-1{list-style-type:none}ul.lst-kix_yf08h6pwef38-2{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_yf08h6pwef38-3{list-style-type:none}ul.lst-kix_yf08h6pwef38-4{list-style-type:none}.lst-kix_m3hcf1tp6dx0-1>li:before{content:"\0025cb   "}ul.lst-kix_yf08h6pwef38-0{list-style-type:none}.lst-kix_m3hcf1tp6dx0-0>li:before{content:"\0025cf   "}ul.lst-kix_a8g0yfe41h5n-1{list-style-type:none}ul.lst-kix_a8g0yfe41h5n-2{list-style-type:none}ul.lst-kix_a8g0yfe41h5n-3{list-style-type:none}ul.lst-kix_a8g0yfe41h5n-4{list-style-type:none}ul.lst-kix_a8g0yfe41h5n-0{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c20{background-color:#ffffff;color:#212529;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c3{margin-left:72pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{background-color:#ffffff;color:#212529;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c29{background-color:#ffffff;color:#212529;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c10{background-color:#ffffff;color:#212529;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c24{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c26{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c28{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c32{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c17{color:#0e0e0e;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c14{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none;font-size:10.5pt}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c23{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c11{font-size:10.5pt;color:#0e0e0e;font-weight:700}.c31{background-color:#ffffff;color:#212529;font-weight:700}.c22{margin-left:50pt;text-indent:-25pt}.c13{color:inherit;text-decoration:inherit}.c27{color:#0e0e0e;font-size:10.5pt}.c12{padding:0;margin:0}.c30{font-size:12pt}.c33{font-size:10.5pt}.c25{height:16pt}.c18{margin-left:36pt}.c21{padding-left:0pt}.c15{margin-left:72pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c23 doc-content"><h2 class="c24" id="h.qypg6of28546"><span class="c16">Enhanced Object Detection, Tracking, and Re-Identification for High-Altitude Aerial Surveillance applications</span></h2><p class="c1"><span class="c7"></span></p><p class="c1"><span class="c19"></span></p><p class="c1"><span class="c19"></span></p><p class="c1"><span class="c19"></span></p><p class="c1"><span class="c19"></span></p><p class="c1"><span class="c19"></span></p><p class="c1"><span class="c19"></span></p><p class="c1"><span class="c19"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c6"><span class="c4">Ahmed Hammad</span></p><p class="c6"><span class="c4">Hiba Imran</span></p><p class="c6"><span class="c4">Shizra Burney</span></p><p class="c1"><span class="c4"></span></p><h2 class="c24 c25" id="h.d0p743ovlrsh"><span class="c5"></span></h2><h2 class="c24" id="h.3vutm2lzp76x"><span class="c5">Introduction</span></h2><p class="c1"><span class="c2"></span></p><p class="c6"><span class="c9">In recent years, advancements in artificial intelligence (AI) and computer vision have significantly improved the capabilities of aerial surveillance systems. These technologies are now being employed in various applications, including border security, traffic monitoring, disaster management, and urban planning. The primary focus of this project is to develop a system for the detection, identification, and re-identification of persons and vehicles from aerial imagery. This task is challenging due to the high variability in appearance caused by different angles, lighting conditions, and occlusions. However, the successful implementation of such a system could have profound implications for enhancing security measures and optimizing resource allocation in large-scale monitoring operations.</span></p><p class="c1"><span class="c8"></span></p><h2 class="c24" id="h.bukbci7ohsco"><span class="c5">Abstract</span></h2><p class="c1"><span class="c2"></span></p><p class="c6"><span class="c9">The detection, identification, and re-identification of persons and vehicles from aerial imagery is a complex and critical task in the field of surveillance and monitoring. This project proposes a novel approach that integrates advanced artificial intelligence and computer vision techniques to overcome the challenges associated with aerial surveillance, such as varying angles, lighting conditions, and occlusions. By employing deep learning models, the system is designed to accurately detect and track objects in real-time, ensuring high reliability and performance. The proposed solution is evaluated on a dataset of aerial images, demonstrating its potential to significantly enhance the efficiency and effectiveness of aerial surveillance systems.</span></p><h2 class="c24" id="h.ej23bghhmh4i"><span class="c5">Problem Statement/Scope</span></h2><p class="c1"><span class="c2"></span></p><p class="c6"><span class="c9">The increasing need for effective surveillance and monitoring systems has driven the demand for advanced techniques that can accurately detect and identify objects, particularly persons and vehicles, in aerial imagery. Traditional surveillance methods are often limited by the resolution of the images and the complexity of the environment, making it difficult to achieve consistent and reliable results. The problem is further compounded when it comes to re-identification, where the system must recognize the same object across different frames or cameras. This project aims to address these challenges by leveraging state-of-the-art AI models and computer vision techniques to develop a robust system capable of detecting, identifying, and re-identifying objects in real-time from aerial images.</span></p><p class="c1"><span class="c8"></span></p><h2 class="c24" id="h.5hefjzo6pffo"><span class="c5">Literature Review(so far)</span></h2><p class="c6"><span class="c9">1.</span></p><p class="c1"><span class="c9"></span></p><p class="c6"><span class="c9">It highlights the challenges of detecting objects with arbitrary orientations in aerial images and discusses existing approaches like Faster R-CNN and Region of Interest (RoI) Transformer. It also emphasizes the importance of using oriented bounding boxes (OBBs) for accurate object detection in complex scenarios.</span></p><p class="c1"><span class="c9"></span></p><p class="c6"><span class="c9">2. https://www.intechopen.com/chapters/67472</span></p><p class="c1"><span class="c9"></span></p><p class="c6"><span class="c9">It covers advancements in person and vehicle re-identification, emphasizing deep learning techniques like Convolutional Neural Networks (CNNs) for feature extraction and metric learning methods for improving accuracy. It also highlights the use of fusion features, combining appearance with spatio-temporal data for enhanced re-identification performance.</span></p><p class="c6"><span class="c9">3. </span></p><p class="c1"><span class="c9"></span></p><p class="c6"><span class="c9">It highlights the limitations of generic object detection methods, particularly in handling small and sparsely distributed objects in aerial images. It introduces the ClusDet network, which uses adaptive clustering and scale estimation to improve detection efficiency and accuracy in aerial image detection.</span></p><p class="c1"><span class="c8"></span></p><p class="c1"><span class="c8"></span></p><p class="c1"><span class="c8"></span></p><h2 class="c24 c25" id="h.z7qa38cccpw2"><span class="c5"></span></h2><h2 class="c24 c25" id="h.85y76kepp2xu"><span class="c5"></span></h2><h2 class="c24 c25" id="h.fgmbkzzec8eg"><span class="c5"></span></h2><h2 class="c24 c25" id="h.b5vvo4by5oc"><span class="c5"></span></h2><h2 class="c24" id="h.2452mgqj0th9"><span class="c5">Block Diagram</span></h2><p class="c1"><span class="c4"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 710.50px; height: 439.00px;"><img alt="" src="images/image7.png" style="width: 710.50px; height: 439.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c4"></span></p><h2 class="c24 c25" id="h.qxm6325aca1"><span class="c7"></span></h2><h2 class="c24 c25" id="h.2jnepd6xwi34"><span class="c7"></span></h2><h2 class="c24 c25" id="h.fphxv0ls20mn"><span class="c7"></span></h2><h2 class="c24" id="h.cw6j0nkbha9i"><span class="c7">Datasets Discovered so far</span></h2><p class="c6 c22"><span class="c27">&bull;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c11">DOTA-v2.0</span><span class="c17">: Contains 11,268 images with 18 categories and over 1.7 million instances. Adds &ldquo;airport&rdquo; and &ldquo;helipad&rdquo; categories.</span></p><p class="c6 c22"><span class="c17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 192.50px; height: 180.47px;"><img alt="" src="images/image4.png" style="width: 192.50px; height: 180.47px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 212.83px; height: 180.00px;"><img alt="" src="images/image5.png" style="width: 212.83px; height: 180.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c22"><span class="c17">&nbsp; </span></p><p class="c1 c22"><span class="c17"></span></p><p class="c6"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 221.75px; height: 191.00px;"><img alt="" src="images/image1.png" style="width: 221.75px; height: 191.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 335.50px; height: 191.41px;"><img alt="" src="images/image6.png" style="width: 335.50px; height: 191.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><p class="c1 c18"><span class="c32"></span></p><ul class="c12 lst-kix_jeohxv44ijje-0 start"><li class="c6 c18 c21 li-bullet-0"><span class="c31 c33">&nbsp; &nbsp; &nbsp;</span><span class="c31 c33">ERA (Event Recognition in Aerial videos): </span><span class="c10">consisting of 2,864 videos each with a label from 25 different classes corresponding to an event unfolding 5 seconds.</span></li></ul><p class="c6 c18"><span class="c14"><a class="c13" href="https://www.google.com/url?q=https://lcmou.github.io/ERA_Dataset/&amp;sa=D&amp;source=editors&amp;ust=1724515042868491&amp;usg=AOvVaw27innVJhg1lXuArkVqA2Dw">Dem</a></span><span class="c10">o</span></p><p class="c1 c18"><span class="c10"></span></p><ul class="c12 lst-kix_m3hcf1tp6dx0-0 start"><li class="c6 c18 c21 li-bullet-0"><span class="c29">ManipalUAVid: </span></li></ul><p class="c6 c18"><span class="c10">This dataset contains aerial videos of a closed campus. four-class fine annotation (Greenery, Construction, Road, and Water Bodies) describing the general layout of the scene.</span></p><p class="c6 c18"><span class="c14"><a class="c13" href="https://www.google.com/url?q=https://github.com/uverma/ManipalUAVid&amp;sa=D&amp;source=editors&amp;ust=1724515042868999&amp;usg=AOvVaw1WmvGnvx2-LsFVwo08b3vx">Link</a></span></p><p class="c6 c18"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 230.50px; height: 239.73px;"><img alt="" src="images/image3.png" style="width: 230.50px; height: 239.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c10">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 238.18px; height: 246.39px;"><img alt="" src="images/image2.png" style="width: 238.18px; height: 246.39px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c10"></span></p><ul class="c12 lst-kix_xlh8jlaybxo-0 start"><li class="c6 c18 c21 li-bullet-0"><span class="c29">Vis-Drone: </span></li></ul><p class="c6 c18"><span class="c10">VisDrone is composed of 288 video clips with 261,908 frames and 10,209 static images. These frames are manually annotated with over 2.6 million bounding boxes of targets such as pedestrians, cars, bicycles, and tricycles. Attributes like scene visibility, object class, and occlusion are also provided for better data utilization.</span></p><p class="c6 c18"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 559.50px; height: 275.00px;"><img alt="" src="images/image8.png" style="width: 559.50px; height: 275.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c18"><span class="c14"><a class="c13" href="https://www.google.com/url?q=https://docs.ultralytics.com/datasets/detect/visdrone/&amp;sa=D&amp;source=editors&amp;ust=1724515042869562&amp;usg=AOvVaw1e3mVZtneso-bV_MduIwaI">LINK</a></span></p><p class="c1 c18"><span class="c10"></span></p><p class="c1 c18"><span class="c10"></span></p><p class="c1 c15"><span class="c10"></span></p><p class="c1 c15"><span class="c10"></span></p><p class="c1"><span class="c10"></span></p><p class="c1"><span class="c10"></span></p><p class="c1"><span class="c10"></span></p><h2 class="c24" id="h.n2q69bpwqq0d"><span class="c5">Models in DOTA research paper</span></h2><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">The main model discussed in the paper for evaluating the DOTA dataset is the Oriented R-CNN. It is an adaptation of Faster R-CNN that is designed specifically for detecting oriented bounding boxes (OBBs). This model incorporates several key modifications to better handle the orientation of objects, which is crucial for tasks involving aerial images and other scenarios where objects can appear in various orientations.</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c20">1.RetinaNet: </span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">A single-stage object detector that uses a focal loss function to handle class imbalance, making it effective for detecting objects in images with varying densities.</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c30 c31">2.Mask R-CNN:</span><span class="c0">&nbsp;An extension of Faster R-CNN that adds a branch for predicting object masks, enabling instance segmentation along with object detection.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c20">3.Cascade Mask R-CNN: </span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">An improvement on Mask R-CNN, which uses a cascade of detectors with increasing IoU (Intersection over Union) thresholds to improve detection accuracy, especially for challenging cases.</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c20">4.Hybrid Task Cascade:</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">&nbsp;A framework that integrates multiple detection tasks, including object detection and instance segmentation, using a cascade approach to enhance performance on both tasks.</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c20">5.Faster R-CNN:</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">&nbsp;Faster R-CNN is an object detection model that improves speed and accuracy by using a Region Proposal Network (RPN) to generate high-quality region proposals. It has two main stages: the RPN, which proposes potential object regions, and Fast R-CNN, which classifies these regions and refines bounding boxes. This design balances accuracy and speed, making it effective for precise object localization.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c20">OTHER MODELS:</span></p><p class="c1"><span class="c20"></span></p><p class="c6"><span class="c20">YOLOv8:</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">YOLOv8 is the latest iteration of the YOLO object detection models, designed for real-time performance. It uses a single neural network to analyze an entire image in one go, predicting bounding boxes and class probabilities for detected objects. The model employs a convolutional neural network (CNN) for feature extraction and prediction.</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c20">SSD:</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">SSD (Single Shot MultiBox Detector) is an object detection model designed for fast and accurate detection of objects in images. It operates in a single pass through the network, unlike older models that required multiple stages.</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c20">SAHI:</span></p><p class="c1"><span class="c0"></span></p><p class="c6"><span class="c0">SAHI(Slicing Aided Hyper Inference) is a method designed to enhance the performance of object detection models on large images by breaking them into smaller, more manageable slices.</span></p><p class="c6"><span class="c0">SAHI improves inference efficiency and accuracy by dividing large images into smaller sections, processing each slice individually, and then combining the results.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c10"></span></p><p class="c1"><span class="c10"></span></p><h2 class="c24" id="h.xi7d3jovlj1f"><span class="c28">Steps Going forward</span></h2><ol class="c12 lst-kix_lo0h0uhovqi2-0 start" start="1"><li class="c6 c18 c21 li-bullet-0"><span class="c30">&nbsp;</span><span class="c26">Dataset Acquisition and Annotation</span></li></ol><ul class="c12 lst-kix_lo0h0uhovqi2-1 start"><li class="c3 li-bullet-0"><span class="c9">Gathering additional datasets , focusing on those with proper annotations.</span></li><li class="c3 li-bullet-0"><span class="c9">Annotate any unlabelled data manually or with semi-automated tools.</span></li><li class="c3 li-bullet-0"><span class="c9">Explore synthetic data generation or augmentation to address dataset limitations, such as the lack of infrared video.</span></li></ul><p class="c1 c15"><span class="c9"></span></p><ol class="c12 lst-kix_lo0h0uhovqi2-0" start="2"><li class="c6 c18 c21 li-bullet-0"><span class="c26">Algorithm Selection &amp; Development</span></li></ol><ul class="c12 lst-kix_lo0h0uhovqi2-1 start"><li class="c3 li-bullet-0"><span class="c9">Choose appropriate algorithms for detection (e.g., YOLO, Faster R-CNN) and re-identification (e.g., triplet networks).</span></li><li class="c3 li-bullet-0"><span class="c9">Design a preprocessing pipeline for the imagery</span></li></ul><p class="c1 c15"><span class="c9"></span></p><ol class="c12 lst-kix_lo0h0uhovqi2-0" start="3"><li class="c6 c18 c21 li-bullet-0"><span class="c26">Model Training and Validation</span></li></ol><p class="c1 c18"><span class="c26"></span></p><ol class="c12 lst-kix_lo0h0uhovqi2-0" start="4"><li class="c6 c18 c21 li-bullet-0"><span class="c26">System Integration and Testing</span></li></ol><ul class="c12 lst-kix_lo0h0uhovqi2-1 start"><li class="c3 li-bullet-0"><span class="c9">Initial integration of the entire process on PC in phase 1</span></li><li class="c3 li-bullet-0"><span class="c9">Phase 2 would require implementation on edge devices.</span></li></ul><p class="c1 c15"><span class="c9"></span></p><ol class="c12 lst-kix_lo0h0uhovqi2-0" start="5"><li class="c6 c18 c21 li-bullet-0"><span class="c26">Documentation and Final Report</span></li></ol><ul class="c12 lst-kix_lo0h0uhovqi2-1 start"><li class="c3 li-bullet-0"><span class="c9">Documenting the entire process, including methodology, results, challenges, and limitations.</span></li></ul><p class="c1 c15"><span class="c9"></span></p><h2 class="c24 c25" id="h.94lcbkqcqoox"><span class="c5"></span></h2><h2 class="c24 c25" id="h.s13iqprebeaq"><span class="c5"></span></h2><h2 class="c24" id="h.agkr4x9pzfzv"><span class="c5">Conclusion</span></h2><p class="c1"><span class="c9"></span></p><p class="c6"><span class="c9">This project successfully addresses the key challenges associated with the detection, identification, and re-identification of persons and vehicles in aerial imagery. Through the integration of cutting-edge AI and computer vision techniques, the developed system has shown promising results in terms of accuracy, speed, and robustness. The implications of this work extend beyond surveillance, offering potential applications in areas such as disaster management, traffic monitoring, and urban planning. Future work could involve further optimizing the models for real-time performance and expanding the system&#39;s capabilities to include other types of objects or scenes. The advancements made in this project pave the way for more intelligent and efficient aerial monitoring systems that can significantly enhance security and resource management.</span></p><p class="c1"><span class="c9"></span></p><p class="c1"><span class="c2"></span></p><p class="c1 c15"><span class="c9"></span></p><p class="c1 c18"><span class="c4"></span></p><p class="c1 c18"><span class="c10"></span></p><div><p class="c1"><span class="c4"></span></p></div></body></html>